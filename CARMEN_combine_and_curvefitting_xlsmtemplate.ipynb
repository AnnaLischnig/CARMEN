{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75046cae",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Step 1: Group CSV Files by Image and Combine into Excel\n",
    "-------------------------------------------------\n",
    "\n",
    "This script combines raw ROI mean intensity values from four-channel calcium imaging experiments \n",
    "processed using the CARMEN Fiji macro (CARMEN_MeasureFourChannelROI.ijm). \n",
    "It scans a folder of CSV exports (one file per image and channel), groups them by base name, \n",
    "and merges the values into a single structured Excel file.\n",
    "\n",
    "Functionality:\n",
    "- Scans a folder for CSV files matching the naming convention (e.g., \"Image01_Intensity FP1.csv\").\n",
    "- Groups CSVs by image base name, ensuring each group contains exactly four files (FP1 to FP4).\n",
    "- Loads each CSV and writes its data into a separate sheet in an Excel file.\n",
    "- Saves the output as \"<base_name>_analysis.xlsx\" in the same folder.\n",
    "\n",
    "Assumptions:\n",
    "- CSV files follow the naming convention: \"<base_name>_Intensity FPX.csv\".\n",
    "- Each CSV represents one fluorescence channel (BFP, GFP, RFP, NIR).\n",
    "- Files are stored together in a single input directory, set by `file_path`.\n",
    "\n",
    "Outputs:\n",
    "- One Excel file per image group: \"<base_name>_analysis.xlsx\"\n",
    "  - Contains four sheets: FP1, FP2, FP3, FP4 — each corresponding to one channel.\n",
    "\n",
    "Usage:\n",
    "- Set the `file_path` variable to the folder containing your CARMEN CSV exports.\n",
    "- Run the script in a Python environment with `pandas` installed.\n",
    "\"\"\"\n",
    "import pandas as pd\n",
    "import os\n",
    "from collections import defaultdict\n",
    "\n",
    "# Define the path to your CSV files\n",
    "file_path = r\"your\\file\\path\"  # Replace with actual path to your CARMEN CSV output\n",
    "\n",
    "# Set suffix for the resulting Excel files\n",
    "excel_suffix = '_analysis'\n",
    "\n",
    "# Initialize dictionary to group files by base name (i.e., per image)\n",
    "grouped_files = defaultdict(list)\n",
    "\n",
    "# Group CSVs by base name: e.g., Image01 from Image01_FP1.csv, Image01_FP2.csv, etc.\n",
    "for file in os.listdir(file_path):\n",
    "    if file.endswith(\".csv\"):\n",
    "        # Extract base name (before _FP1, _FP2, etc.)\n",
    "        base_name = file.rsplit('_', 1)[0]\n",
    "        grouped_files[base_name].append(file)\n",
    "\n",
    "# Optional: track the created Excel file paths\n",
    "excel_file_paths = []\n",
    "\n",
    "# Loop through each group and combine into an Excel file\n",
    "for base_name, files in grouped_files.items():\n",
    "    # Only proceed if there are exactly 4 files (assumes 4 fluorescence planes)\n",
    "    if len(files) == 4:\n",
    "        # Sort files to enforce FP1 → FP4 order\n",
    "        files.sort(key=lambda x: int(x[-5]))  # assumes last digit before '.csv' is 1–4\n",
    "\n",
    "        # Load each CSV into a dictionary with keys like 'FP1', 'FP2', etc.\n",
    "        csv_files = {}\n",
    "        for file in files:\n",
    "            suffix = file.rsplit('_', 1)[1].split('.')[0]  # Extract FP1, FP2...\n",
    "            full_path = os.path.join(file_path, file)\n",
    "            csv_files[suffix] = pd.read_csv(full_path)\n",
    "\n",
    "        # Create a new Excel workbook for this image\n",
    "        output_excel_path = os.path.join(file_path, f'{base_name}{excel_suffix}.xlsx')\n",
    "        with pd.ExcelWriter(output_excel_path) as writer:\n",
    "            for sheet_name, df in csv_files.items():\n",
    "                # Each CSV becomes one sheet in the Excel file\n",
    "                df.to_excel(writer, sheet_name=sheet_name, index=False)\n",
    "\n",
    "        print(f\"CSV files for {base_name} successfully saved to {base_name}{excel_suffix}.xlsx.\")\n",
    "    \n",
    "    else:\n",
    "        # Notify if a group doesn't have the expected 4 files\n",
    "        print(f\"Warning: {base_name} does not have exactly 4 corresponding CSV files.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6d436a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Step 2: Load Template, Transfer Data from CARMEN Sheets, and Clean Up\n",
    "-----------------------------------------------------------------\n",
    "\n",
    "This script takes the structured Excel output files from STEP 1 and copies\n",
    "their contents into a macro-enabled Excel template (CARMEN_template.xlsm). It maps each \n",
    "fluorescence channel to a specific sheet in the template and prepares the data for further \n",
    "processing such as bleach curve fitting.\n",
    "\n",
    "Functionality:\n",
    "- Opens each \"<base_name>_analysis.xlsx\" file and the CARMEN Excel template.\n",
    "- Copies headers and ROI intensity data from FP1 to FP4 into the corresponding template sheets:\n",
    "  - FP1 → BFP\n",
    "  - FP2 → GFP\n",
    "  - FP3 → RFP\n",
    "  - FP4 → NIR\n",
    "- Additionally copies the last column of each ROI dataset (e.g., background or selected value) into column 'BD'.\n",
    "- Cleans up all template sheets by deleting rows below the last valid data row.\n",
    "- Saves a new macro-enabled Excel file: \"<base_name>_analysis_output.xlsm\".\n",
    "\n",
    "Assumptions:\n",
    "- Input files follow the naming convention: \"<base_name>_analysis.xlsx\".\n",
    "- A working macro-enabled Excel template file (CARMEN_template.xlsm) is available.\n",
    "- Data in each input sheet begins at cell C1 (headers) and C2 (data).\n",
    "- Output sheets in the template are named 'BFP', 'GFP', 'RFP', and 'NIR'.\n",
    "\n",
    "Outputs:\n",
    "- For each image group, saves: \"<base_name>_analysis_output.xlsm\"\n",
    "  - Structured and cleaned Excel template with inserted raw intensity data.\n",
    "\n",
    "Usage:\n",
    "- Set `template_folder` and `results_folder` to the correct paths.\n",
    "- Ensure `CARMEN_template.xlsm` exists and ends with `.xlsm`.\n",
    "- Run the script in a Python environment with `xlwings` installed.\n",
    "\"\"\"\n",
    "\n",
    "import xlwings as xw\n",
    "import os\n",
    "\n",
    "# Define paths – adjust to your directory structure\n",
    "template_folder = r\"your\\file\\path\\template\"\n",
    "results_folder = r\"your\\file\\path\"\n",
    "template_file = os.path.join(template_folder, \"CARMEN_template.xlsm\")\n",
    "\n",
    "# Ensure the template file ends with \".xlsm\" for macro compatibility\n",
    "\n",
    "# Loop through all result Excel files that match the CARMEN format\n",
    "for file_name in os.listdir(results_folder):\n",
    "    if file_name.endswith(\"_analysis.xlsx\"):\n",
    "        data_file_path = os.path.join(results_folder, file_name)\n",
    "\n",
    "        # Launch Excel in background mode (no UI, no alerts)\n",
    "        app = xw.App(visible=False)\n",
    "        app.display_alerts = False\n",
    "        app.screen_updating = False\n",
    "\n",
    "        # Open the analysis result file and the template workbook\n",
    "        template = app.books.open(template_file)\n",
    "        data_file = app.books.open(data_file_path)\n",
    "\n",
    "        # Map raw data sheets (FP1–FP4) to their corresponding template target sheets\n",
    "        sheet_mappings = {\n",
    "            \"Intensity FP1\": \"BFP\",\n",
    "            \"Intensity FP2\": \"GFP\",\n",
    "            \"Intensity FP3\": \"RFP\",\n",
    "            \"Intensity FP4\": \"NIR\"\n",
    "        }\n",
    "\n",
    "        # Track the deepest row of data to clean up template below it\n",
    "        last_data_row = 0\n",
    "\n",
    "        for data_sheet, template_sheet in sheet_mappings.items():\n",
    "            try:\n",
    "                # Check if expected sheet is present in the analysis file\n",
    "                if data_sheet not in [sheet.name for sheet in data_file.sheets]:\n",
    "                    print(f\"Sheet '{data_sheet}' not found in {file_name}\")\n",
    "                    continue\n",
    "\n",
    "                # Read header and data range from source sheet\n",
    "                data_sheet_obj = data_file.sheets[data_sheet]\n",
    "                headers = data_sheet_obj.range('C1').value  # Assumes header starts at C1\n",
    "                data_range = data_sheet_obj.range('C2').expand('table').value  # Assumes data starts at C2\n",
    "\n",
    "                if not data_range:\n",
    "                    print(f\"No data found in {data_sheet} in {file_name}\")\n",
    "                    continue\n",
    "\n",
    "                # Track the longest data section for cleanup purposes\n",
    "                last_data_row = max(last_data_row, len(data_range) + 1)\n",
    "\n",
    "                # Write header and data to corresponding sheet in the template (starting in column G)\n",
    "                template_sheet_obj = template.sheets[template_sheet]\n",
    "                template_sheet_obj.range('G1').value = [headers]           # Headers in row 1\n",
    "                template_sheet_obj.range('G2').value = data_range          # Data starting in row 2\n",
    "\n",
    "                # Copy last column of each data row (e.g., background or ROI max) into column BD\n",
    "                last_col_data = [row[-1] for row in data_range]\n",
    "                template_sheet_obj.range('BD2').value = [[val] for val in last_col_data]\n",
    "\n",
    "                print(f\"Processed {data_sheet} in {file_name}\")\n",
    "\n",
    "            except Exception as e:\n",
    "                print(f\" Error processing {data_sheet} in {file_name}: {e}\")\n",
    "\n",
    "        #  Cleanup: Remove leftover rows from all sheets below the data region\n",
    "        for template_sheet in template.sheets:\n",
    "            try:\n",
    "                max_rows = template_sheet.api.UsedRange.Rows.Count\n",
    "                if last_data_row < max_rows:\n",
    "                    # Delete extra rows to avoid miscalculations or leftover formulas\n",
    "                    template_sheet.api.Rows(f\"{last_data_row + 1}:{max_rows}\").Delete()\n",
    "                print(f\"Cleaned extra rows in sheet '{template_sheet.name}'\")\n",
    "            except Exception as e:\n",
    "                print(f\" Error cleaning rows in sheet '{template_sheet.name}': {e}\")\n",
    "\n",
    "        #  Save template as new file, preserving macros and structure\n",
    "        output_file_path = os.path.join(results_folder, f\"{os.path.splitext(file_name)[0]}_output.xlsm\")\n",
    "        template.save(output_file_path)\n",
    "\n",
    "        #  Close files and Excel instance\n",
    "        template.close()\n",
    "        data_file.close()\n",
    "        app.quit()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bba8b702",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "STEP 3: CARMEN Calcium Multiplexing Bleaching Correction and Curve Fitting\n",
    "-------------------------------------------------------------------\n",
    "\n",
    "This script performs exponential decay fitting on time course data from four-channel calcium imaging experiments \n",
    "prepared using the CARMEN Fiji macro and transferred into structured Excel templates. It assumes that each image \n",
    "has already been processed through background subtraction and inserted into a macro-enabled Excel file.\n",
    "\n",
    "Functionality:\n",
    "- Opens each structured `.xlsm` file and reads the 'Bleaching' sheet containing time and signal data.\n",
    "- Scales time values to improve numerical stability during fitting.\n",
    "- Removes local outliers using a sliding window method.\n",
    "- Fits an exponential decay model to each ROI time trace using `scipy.optimize.curve_fit`.\n",
    "- Writes the fitted curves into corresponding output sheets (BFP, GFP, RFP, NIR) starting from column 'DF'.\n",
    "\n",
    "Assumptions:\n",
    "- Excel files follow the naming convention: \"<base_name>_analysis_output.xlsm\".\n",
    "- The 'Bleaching' sheet contains time data in column A and signal intensities in defined column blocks:\n",
    "  - BFP: columns D to BA\n",
    "  - GFP: columns BC to CZ\n",
    "  - RFP: columns DB to EY\n",
    "  - NIR: columns FA to GX\n",
    "- Time values are in seconds or another uniform unit and appear without missing rows.\n",
    "- The last row of valid data is automatically detected (NaNs, empty cells mark the end).\n",
    "- The Excel template already contains output sheets named 'BFP', 'GFP', 'RFP', and 'NIR'.\n",
    "\n",
    "Outputs:\n",
    "- For each input file, overwrites the original .xlsm file with:\n",
    "  - Fitted decay curves for each ROI written into the respective channel sheet starting from column 'DF'.\n",
    "  - Cleaned-up 'Bleaching' sheet with extraneous rows removed below valid data.\n",
    "\n",
    "Usage:\n",
    "- Manually inspect and adjust time windows and bleach start in the Excel file before running.\n",
    "- Set `results_folder` to the path containing your CARMEN `.xlsm` files.\n",
    "- Run the script in a Python environment with `xlwings`, `numpy`, and `scipy` installed.\n",
    "\"\"\"\n",
    "\n",
    "import xlwings as xw\n",
    "import os\n",
    "import numpy as np\n",
    "from scipy.optimize import curve_fit\n",
    "\n",
    "# Define folder containing the .xlsm files from previous step\n",
    "results_folder = r\"your\\file\\path\"\n",
    "\n",
    "# Define exponential decay function for curve fitting\n",
    "def exp_decay(x, a, b, c):\n",
    "    return a * np.exp(-b * x) + c\n",
    "\n",
    "# Local outlier removal: compares each point to surrounding values in a window\n",
    "def remove_outliers_local_window(data, window_size=3, threshold=2.5):\n",
    "    filtered_data = data.copy()\n",
    "    n = len(data)\n",
    "    \n",
    "    for i in range(n):\n",
    "        start = max(0, i - window_size)\n",
    "        end = min(n, i + window_size + 1)\n",
    "        surrounding_points = np.concatenate([data[start:i], data[i+1:end]])\n",
    "        surrounding_points = surrounding_points[np.isfinite(surrounding_points)]  # Ignore NaNs\n",
    "        \n",
    "        if len(surrounding_points) > 0:\n",
    "            local_mean = np.mean(surrounding_points)\n",
    "            local_std = np.std(surrounding_points)\n",
    "            if abs(data[i] - local_mean) > threshold * local_std:\n",
    "                filtered_data[i] = np.nan\n",
    "\n",
    "    return filtered_data\n",
    "\n",
    "# Loop through each .xlsm file that contains bleaching data to fit\n",
    "for file_name in os.listdir(results_folder):\n",
    "    if file_name.endswith(\"_analysis_output.xlsm\"):\n",
    "        data_file_path = os.path.join(results_folder, file_name)\n",
    "\n",
    "        # Launch Excel invisibly\n",
    "        app = xw.App(visible=False)\n",
    "        app.display_alerts = False\n",
    "        app.screen_updating = False\n",
    "\n",
    "        workbook = app.books.open(data_file_path)\n",
    "\n",
    "        try:\n",
    "            # Access the bleaching data sheet\n",
    "            bleaching_sheet = workbook.sheets['Bleaching']\n",
    "\n",
    "            # Extract time data from column A\n",
    "            time_data = bleaching_sheet.range('A2:A' + str(bleaching_sheet.cells.last_cell.row)).value\n",
    "            time_data = [value if isinstance(value, (int, float)) else np.nan for value in time_data]\n",
    "            time_data = np.array(time_data, dtype=float)\n",
    "\n",
    "            # Determine last valid data row\n",
    "            last_data_row = len(time_data)\n",
    "            for i, value in enumerate(time_data):\n",
    "                if np.isnan(value):\n",
    "                    last_data_row = i\n",
    "                    break\n",
    "            time_data = time_data[:last_data_row]\n",
    "\n",
    "            # Normalise time values for numerical stability\n",
    "            time_data_max = np.max(time_data)\n",
    "            time_data_scaled = time_data / time_data_max if time_data_max > 0 else time_data\n",
    "\n",
    "            # Clean up rows below the data range\n",
    "            max_rows = bleaching_sheet.cells.last_cell.row\n",
    "            if last_data_row < max_rows:\n",
    "                bleaching_sheet.api.Rows(f\"{last_data_row + 2}:{max_rows}\").Delete()\n",
    "\n",
    "            # Define input data ranges for each channel (column start → end)\n",
    "            fp_data_ranges = {\n",
    "                'BFP': ('D', 'BA'),\n",
    "                'GFP': ('BC', 'CZ'),\n",
    "                'RFP': ('DB', 'EY'),\n",
    "                'NIR': ('FA', 'GX')\n",
    "            }\n",
    "\n",
    "            # Mapping: output sheet name for each fluorescence protein\n",
    "            fp_sheets = {\n",
    "                'BFP': 'BFP',\n",
    "                'GFP': 'GFP',\n",
    "                'RFP': 'RFP',\n",
    "                'NIR': 'NIR'\n",
    "            }\n",
    "\n",
    "            # Loop through each channel and fit decay curves column-wise\n",
    "            for fp, (start_col, end_col) in fp_data_ranges.items():\n",
    "                data_range = bleaching_sheet.range(f'{start_col}2:{end_col}{last_data_row + 1}').value\n",
    "                data_range = [[val if isinstance(val, (int, float)) else np.nan for val in row] for row in data_range]\n",
    "                data_range = np.array(data_range, dtype=float)\n",
    "                if data_range.ndim == 1:\n",
    "                    data_range = data_range[:, np.newaxis]\n",
    "\n",
    "                output_sheet = workbook.sheets[fp_sheets[fp]]\n",
    "                save_start_col = 110  # Column DF (0-based index)\n",
    "\n",
    "                for col_idx in range(data_range.shape[1]):\n",
    "                    col_data = data_range[:, col_idx]\n",
    "                    save_col = save_start_col + col_idx\n",
    "\n",
    "                    # Remove local outliers\n",
    "                    col_data_no_outliers = remove_outliers_local_window(col_data)\n",
    "\n",
    "                    # Skip non-positive or empty columns\n",
    "                    if np.all(col_data_no_outliers <= 0):\n",
    "                        print(f\"Skipping column index {col_idx} in {fp_sheets[fp]} due to non-positive values\")\n",
    "                        output_sheet.range((2, save_col)).clear_contents()\n",
    "                        continue\n",
    "\n",
    "                    # Mask valid points for fitting\n",
    "                    valid_mask = np.isfinite(time_data) & np.isfinite(col_data_no_outliers)\n",
    "                    valid_time = time_data_scaled[valid_mask]\n",
    "                    valid_col_data = col_data_no_outliers[valid_mask]\n",
    "\n",
    "                    # Ensure sufficient data points\n",
    "                    if len(valid_time) < 2 or len(valid_col_data) < 2:\n",
    "                        print(f\"Skipping column index {col_idx} in {fp_sheets[fp]} due to insufficient valid data\")\n",
    "                        output_sheet.range((2, save_col)).clear_contents()\n",
    "                        continue\n",
    "\n",
    "                    # Fit exponential decay to valid data points\n",
    "                    try:\n",
    "                        p0 = [400, 0.003, 300]  # Initial guess for [a, b, c]\n",
    "                        bounds = ([0, 0, -np.inf], [np.inf, np.inf, np.inf])  # Parameter bounds\n",
    "                        popt, _ = curve_fit(exp_decay, valid_time, valid_col_data, p0=p0, maxfev=10000, bounds=bounds)\n",
    "                        fitted_curve = exp_decay(time_data_scaled, *popt)\n",
    "                        fitted_curve = [val if np.isfinite(val) else '' for val in fitted_curve]\n",
    "\n",
    "                        # Write fitted data to output sheet\n",
    "                        output_sheet.range((2, save_col)).value = [[val] for val in fitted_curve]\n",
    "\n",
    "                    except RuntimeError as e:\n",
    "                        print(f\"Fit failed for column index {col_idx} in {fp_sheets[fp]}: {e}\")\n",
    "                        output_sheet.range((2, save_col)).clear_contents()\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\" Error processing file {file_name}: {e}\")\n",
    "        finally:\n",
    "            # Save and close workbook\n",
    "            workbook.save()\n",
    "            workbook.close()\n",
    "            app.quit()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
