{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load CSV and combine to excel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "STEP1: Group CSVs into Excel Files\n",
    "----------------------------------------------------------------\n",
    "\n",
    "This script processes per-channel CSV files exported from a calcium imaging experiment and\n",
    "combines them into structured Excel files, one per image. It assumes that each image has\n",
    "four associated channels (e.g., FB1 to FB4), stored in individual CSVs with a shared base name.\n",
    "\n",
    "Functionality:\n",
    "- Scans a folder for all `.csv` files.\n",
    "- Groups files by image base name, expecting four channels per image.\n",
    "- Sorts channel files to ensure consistent FP1 to FP4 order.\n",
    "- Loads CSV data and writes each channel to a separate sheet in a single Excel file.\n",
    "\n",
    "Assumptions:\n",
    "- CSVs are named using the format \"<base_name>_FBX.csv\", where X ∈ {1,2,3,4}.\n",
    "- All CSV files are located in the same directory (`file_path`).\n",
    "\n",
    "Outputs:\n",
    "- One Excel file per image: \"<base_name>_analysis.xlsx\"\n",
    "  - Sheets: FB1, FB2, FB3, FB4 — raw per-channel intensity data.\n",
    "\n",
    "Usage:\n",
    "- Update `file_path` to point to the directory containing the CSV files.\n",
    "- Run in a Python environment with `pandas` installed.\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "import pandas as pd\n",
    "import os\n",
    "from collections import defaultdict\n",
    "\n",
    "# Define the path to your CSV files\n",
    "file_path = r\"your\\file\\path\"  # Replace with the actual path to your files\n",
    "\n",
    "# Set the suffix for the output Excel file\n",
    "excel_suffix = '_analysis'\n",
    "\n",
    "# Dictionary to hold the grouped CSV files\n",
    "grouped_files = defaultdict(list)\n",
    "\n",
    "# Loop through the files in the folder\n",
    "for file in os.listdir(file_path):\n",
    "    if file.endswith(\".csv\"):\n",
    "        # Split the file name into base and suffix (FB1, FB2, etc.)\n",
    "        base_name = file.rsplit('_', 1)[0]  # Get the part before _FB1, _FB2, etc.\n",
    "        grouped_files[base_name].append(file)\n",
    "\n",
    "# List to hold paths of all created Excel files\n",
    "excel_file_paths = []\n",
    "\n",
    "# Process each group of files\n",
    "for base_name, files in grouped_files.items():\n",
    "    # Ensure there are exactly 4 files for this base name\n",
    "    if len(files) == 4:\n",
    "        # Sort files to ensure FB1, FB2, FB3, FB4 order\n",
    "        files.sort(key=lambda x: int(x[-5]))\n",
    "\n",
    "        # Dictionary to hold the CSV data\n",
    "        csv_files = {}\n",
    "        \n",
    "        # Load the CSV files into the dictionary\n",
    "        for file in files:\n",
    "            suffix = file.rsplit('_', 1)[1].split('.')[0]  # Get FB1, FB2, etc.\n",
    "            full_path = os.path.join(file_path, file)\n",
    "            csv_files[suffix] = pd.read_csv(full_path)\n",
    "\n",
    "        # Create an Excel writer object to save all the sheets into one file\n",
    "        output_excel_path = os.path.join(file_path, f'{base_name}{excel_suffix}.xlsx')\n",
    "        with pd.ExcelWriter(output_excel_path) as writer:\n",
    "            for sheet_name, df in csv_files.items():\n",
    "                df.to_excel(writer, sheet_name=sheet_name, index=False)\n",
    "\n",
    "        print(f\"CSV files for {base_name} successfully saved to {base_name}{excel_suffix}.xlsx.\")\n",
    "    else:\n",
    "        print(f\"Warning: {base_name} does not have exactly 4 corresponding CSV files.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Combine excel files (Background Is also handeled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "STEP 2: Extract Rows from Channel Sheets and Perform Background Subtraction\n",
    "---------------------------------------------------------------------------------\n",
    "\n",
    "This script processes multiple Excel files generated from per-channel calcium imaging data.\n",
    "It extracts intensity values from each file, combines them into a single DataFrame per channel,\n",
    "and performs automated background subtraction.\n",
    "\n",
    "Functionality:\n",
    "- Scans a folder for all \"<base_name>_analysis.xlsx\" files.\n",
    "- Groups files by base name (e.g., different repeats of the same sample).\n",
    "- Extracts the second row of each channel sheet as the data line.\n",
    "- Aligns data across up to 50 'Mean' values and appends 'Background'.\n",
    "- Automatically determines background from the last valid Mean value.\n",
    "- Subtracts this background to create corrected 'BS' (background-subtracted) columns.\n",
    "- Prevents redundancy by setting the column used as background to NaN.\n",
    "\n",
    "Assumptions:\n",
    "- Excel sheets are named \"Intensity FP1\" to \"Intensity FP4\".\n",
    "- The second row of each sheet contains the relevant signal values.\n",
    "- Columns are structured as: [File name, Index, , Label, Mean1 to Mean50].\n",
    "\n",
    "Outputs:\n",
    "- One combined Excel file per base name: \"<base_name>combined_analysis.xlsx\"\n",
    "  - Each sheet contains raw and background-subtracted data.\n",
    "  - Columns: File info + Mean1–Mean50 + Background + BS1–BS50\n",
    "\n",
    "Usage:\n",
    "- Update `results_folder` to point to your Excel input directory.\n",
    "- Run in a Python environment with `pandas` and `numpy` installed.\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Folder containing the Excel files\n",
    "results_folder = r\"your\\file\\path\"  # Replace with the path to your folder  \n",
    "\n",
    "# Define the target headers for the combined file\n",
    "headers = ['File name', 'Index', ' ', 'Label'] + [f'Mean{i}' for i in range(1, 51)] + ['Background']\n",
    "\n",
    "# Loop through the files in the folder to identify unique base names\n",
    "base_name_files = {}\n",
    "for file_name in os.listdir(results_folder):\n",
    "    if file_name.endswith(\".xlsx\") and '_analysis' in file_name:\n",
    "        # Extract the base name (everything before the numbering and analysis suffix)\n",
    "        base_name = file_name.rsplit('_', 2)[0] + '_'  # Improved base name extraction\n",
    "        if base_name not in base_name_files:\n",
    "            base_name_files[base_name] = []\n",
    "        base_name_files[base_name].append(file_name)\n",
    "\n",
    "# Process each set of files with the same base name\n",
    "for base_name, file_names in base_name_files.items():\n",
    "    # Dictionary to hold data for each sheet\n",
    "    combined_data = {\n",
    "        'Intensity FP1': [],\n",
    "        'Intensity FP2': [],\n",
    "        'Intensity FP3': [],\n",
    "        'Intensity FP4': []\n",
    "    }\n",
    "\n",
    "    # Loop through the files with the same base name\n",
    "    for file_name in file_names:\n",
    "        # Load the Excel file\n",
    "        file_path = os.path.join(results_folder, file_name)\n",
    "        try:\n",
    "            xls = pd.ExcelFile(file_path)\n",
    "        except (ValueError, pd.errors.ExcelFileError) as e:\n",
    "            print(f\"Skipping file {file_name} due to error: {e}\")\n",
    "            continue\n",
    "\n",
    "        # Loop through each sheet and extract the second row\n",
    "        for sheet_name in combined_data.keys():\n",
    "            if sheet_name in xls.sheet_names:\n",
    "                df = pd.read_excel(xls, sheet_name=sheet_name, header=None)\n",
    "                if len(df) > 1:\n",
    "                    second_row = df.iloc[1].tolist()  # Extract the second row\n",
    "\n",
    "                    # Ensure the second row has the correct length by padding or trimming\n",
    "                    required_length = len(headers) - 2  # Subtracting 2 to account for 'File name' and 'Index'\n",
    "                    if len(second_row) < required_length:\n",
    "                        second_row.extend([np.nan] * (required_length - len(second_row)))\n",
    "                    elif len(second_row) > required_length:\n",
    "                        second_row = second_row[:required_length]\n",
    "\n",
    "                    # Add the full file name and an index in front of the row\n",
    "                    combined_row = [file_name, len(combined_data[sheet_name]) + 1] + second_row\n",
    "                    combined_data[sheet_name].append(combined_row)\n",
    "\n",
    "    # Create a combined file for the current base name\n",
    "    output_file = os.path.join(results_folder, f'{base_name}combined_analysis.xlsx')\n",
    "    with pd.ExcelWriter(output_file, engine='openpyxl') as writer:\n",
    "        # Write each sheet's combined data to the output file\n",
    "        for sheet_name, data in combined_data.items():\n",
    "            combined_df = pd.DataFrame(data, columns=headers)\n",
    "\n",
    "            # Identify numerical columns (assuming 'Mean1' to 'Mean50' are numerical)\n",
    "            mean_cols = [f'Mean{i}' for i in range(1, 51)]\n",
    "            bs_cols = [f'BS{i}' for i in range(1, 51)]\n",
    "\n",
    "            # Implement the vectorized approach to find the last numerical value\n",
    "            # Reverse the order of numerical columns\n",
    "            reversed_numerical = combined_df[mean_cols].iloc[:, ::-1]\n",
    "\n",
    "            # Perform backward fill along the columns\n",
    "            filled = reversed_numerical.bfill(axis=1)\n",
    "\n",
    "            # Select the first non-NaN value from the reversed columns, which corresponds to the last numerical value in original order\n",
    "            combined_df['Background'] = filled.iloc[:, 0]\n",
    "\n",
    "            # Step 1: Identify the 'MeanX' column used as 'Background' for each row\n",
    "            # This is done by finding the first non-NaN 'Mean' column in the reversed order\n",
    "            combined_df['Background_col'] = combined_df[mean_cols].apply(\n",
    "                lambda row: row[::-1].first_valid_index(), axis=1\n",
    "            )\n",
    "\n",
    "            # Step 2: Subtract 'Background' from all 'Mean' columns to create 'BS' columns\n",
    "            combined_df[bs_cols] = combined_df[mean_cols].subtract(combined_df['Background'], axis=0)\n",
    "\n",
    "            # Step 3: For each row, set the 'BS' column corresponding to 'Background_col' to NaN\n",
    "            # This removes the redundant zero\n",
    "            for i in range(1, 51):\n",
    "                bs_col = f'BS{i}'\n",
    "                mean_col = f'Mean{i}'\n",
    "                combined_df.loc[combined_df['Background_col'] == mean_col, bs_col] = np.nan\n",
    "\n",
    "            # Step 4: (Optional) Drop the 'Background_col' if not needed\n",
    "            combined_df.drop(columns=['Background_col'], inplace=True)\n",
    "\n",
    "            # Optional: Reorder columns to place BS columns after Background\n",
    "            # Define the new column order\n",
    "            new_order = headers + bs_cols\n",
    "            combined_df = combined_df[new_order]\n",
    "\n",
    "            # Write the DataFrame to the Excel sheet\n",
    "            combined_df.to_excel(writer, sheet_name=sheet_name, index=False)\n",
    "\n",
    "    print(f\"Combined file saved as {output_file}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
